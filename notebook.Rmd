
```{R, include=FALSE}
library(knitr)
knitr::opts_chunk$set(warning = FALSE)
```

## Install libraries
```{R, eval = FALSE}
install.packages("caret")
install.packages("glmnet")
install.packages("tidyverse")
install.packages("ggrepel")
install.packages("randomForest")
install.packages("ROCR")
install.packages("kableExtra")
install.packages("vtable")
```

## Load libraries
```{R, message = FALSE}
library("caret")
library("glmnet")
library("tidyverse")
library("dplyr")
library("ggplot2")
library("ggrepel")
library("randomForest")
library("ROCR")
library("kableExtra")
library("vtable")
require(data.table)
```

# Data
Our data consists of three datasets: "male_teams.csv", "male_players (legacy).csv", and "male_coaches.csv". It
contains data on national and club teams from 2015-2023. The data is scraped from the FIFA games produced annually.
While our research concerns real-life players and teams, the stats in the games require teams of observers watching
real matches for months to decide. Thus, we will treat the data as if it applies to the real players and teams.

The dataset can be found at https://www.kaggle.com/datasets/stefanoleone992/fifa-23-complete-player-dataset.

## Load datasets
```{R, eval = TRUE}
teams <- read.csv("./male_teams.csv")
players <- read.csv("./male_players (legacy).csv")
coaches <- read.csv("./male_coaches.csv")
```

## Processing
### Teams
For the teams dataset, we wanted to focus on teams who had more than one coach to gauge the impact each coach left
on the teams overall stats.

Before using any of the data, any columns with NA values in them were disregarded to not impede the models later
on. The Coach_ID column from teams dataset, for example, which is prevelant to answer our questions, instead had 
its NA values replaced with the median of the column.

As such, the teams dataset was reduced from 54 columns to the 27 listed below:

```{R}
newTeams <- data.frame(matrix(ncol = 54, nrow = 0))
colnames(newTeams) <- names(teams)
for (team in unique(teams$team_id)) {
    team_df <- subset(teams, team_id == team)

    if (length(unique(team_df$coach_id)) > 1) {
        newTeams <- rbind(newTeams, team_df)
    }
}

# Replaces NA values in coach_id column with median of coach_id
val <- median(na.omit(newTeams$coach_id))
newTeams$coach_id[is.na(newTeams$coach_id)] <- val

newTeams <- newTeams[, colSums(is.na(newTeams)) == 0] # Remove all columns with any NA values in them
```

``` {R, echo = FALSE}
knitr::kable(names(newTeams), format = "markdown")
```

### Players
The same was done with the players dataset. To prevent using a large dataset, players were reduced to the first
3000 unique defence players. With each player having an entry for each FIFA update, the dataset grows large. each
row then had that defence player's goalkeeper and goalkeeper overall for that FIFA update to help our models later
answer our thrid and fourth questions.

The same treatment of omitting NA columns was done to the players dataset, leaving 105 columns as shown below:
```{R}
modifiedPlayers <- subset(players, club_position %in%
    c("CB", "LB", "RB", "RWB", "LWB", "SW"))

modifiedPlayers <- subset(modifiedPlayers, short_name %in%
    unique(modifiedPlayers$short_name)[1:3000])

gkPlayers <- subset(players, club_position == "GK")

modifiedPlayers["gkName"] <- NA
modifiedPlayers["gkOverall"] <- NA


for (row in seq_len(nrow(modifiedPlayers))) {
    date <- as.Date(modifiedPlayers[row, "club_joined_date"])
    formattedFifaUpdate <- as.Date(modifiedPlayers$fifa_update_date)
    index <- which.min(abs(formattedFifaUpdate - date))

    filterData <- subset(
        gkPlayers,
        as.Date(fifa_update_date) >= as.Date(modifiedPlayers$fifa_update_date[index]) &
            club_team_id == modifiedPlayers[row, "club_team_id"]
    )

    if (nrow(filterData) != 0) {
        modifiedPlayers[row, ]$gkName <- filterData$short_name[1]
        modifiedPlayers[row, ]$gkOverall <- filterData$overall[1]
    }
}

# Replaces NA values in gkOverall column with median of gkOverall
val <- median(na.omit(modifiedPlayers$gkOverall))
modifiedPlayers$gkOverall[is.na(modifiedPlayers$gkOverall)] <- val

modifiedPlayers$gkName[is.na(modifiedPlayers$gkName)] <- "None"

newPlayers <- modifiedPlayers[, colSums(is.na(modifiedPlayers)) == 0] # Remove all columns with any NA values in them

# Remove arithmetic signs from numeric columns
newPlayers[] <- lapply(newPlayers, function(x) {
    gsub("[-|\\+].*", "", x)
})
```

``` {R, echo = FALSE}
knitr::kable(names(newPlayers), format = "markdown")
```

# Questions
1. Do coaches have a big impact on the overall performance of the team they coach?
2. If not, what predictors do, and are they related to coaches at all?
3. Does a defence player's performance have a large impact on their goalkeeper's performance?
4. If not, which predictors do?

# Visualizations
## Coaches and Teams
Let's first explore the current overall of the teams in the dataset, where overall combines their attack, midfield, 
and defence stats into one.

```{R}
st(teams[, c("overall", "attack", "midfield", "defence")],
    labels = c("Overall", "Attack", "Midfield", "Defence"),
    title = "Summary of Team Statistics",
    col.align = "right",
    out = "return"
) %>%
    kbl(caption = "Summary of Team Statistics") %>%
    kable_styling()
```

To see how the teams' overall changes over the tenure of their different coaches, we plot the teams' overalls throughout. 
Here we see an example of Villarreal and Aston Villa teams:

```{R, echo = FALSE}
coachNames <- data.frame(matrix(ncol = 1, nrow = 0))
colnames(coachNames) <- c("name")

for (team in list("Villarreal", "Aston Villa")) {
    team_df <- subset(teams, team_name == team)

    if (length(unique(team_df$coach_id)) <= 1) {
        next
    }

    finalTeamDF <- data.frame(matrix(ncol = 54, nrow = 0))
    colnames(finalTeamDF) <- names(team_df)

    for (i in seq_len(nrow(team_df))) { # Now makes a list of all names first, then uses it in plot
        sname <- c(coaches[which(team_df[i, ]$coach_id == coaches$coach_id), ]$short_name)

        if (identical(sname, character(0))) {
            next
        } else if (nrow(coachNames) == 0) {
            finalTeamDF <- rbind(finalTeamDF, team_df[i, ])

            coachNames[nrow(coachNames) + 1, ] <-
                c(coaches[which(team_df[i, ]$coach_id == coaches$coach_id), ]$short_name)
        } else if (i == nrow(team_df)) {
            finalTeamDF <- rbind(finalTeamDF, team_df[i, ])

            coachNames[nrow(coachNames) + 1, ] <-
                c(coaches[which(team_df[i, ]$coach_id == coaches$coach_id), ]$short_name)
        } else if (tail(coachNames, 1)[1, 1] == sname & nrow(coachNames) > 0) {
            next
        } else {
            finalTeamDF <- rbind(finalTeamDF, team_df[i - 1, ])
            coachNames[nrow(coachNames) + 1, ] <-
                c(coaches[which(team_df[i - 1, ]$coach_id == coaches$coach_id), ]$short_name)

            finalTeamDF <- rbind(finalTeamDF, team_df[i, ])
            coachNames[nrow(coachNames) + 1, ] <-
                c(coaches[which(team_df[i, ]$coach_id == coaches$coach_id), ]$short_name)
        }
    }

    plot <- ggplot(finalTeamDF, aes(finalTeamDF$fifa_update_date, finalTeamDF$overall)) +
        ggtitle("Coach and Team Performance Overall") +
        xlab("Date") +
        ylab("Overall Rating") +
        geom_point() +
        geom_text_repel(aes(label = paste(finalTeamDF$team_name, "/", coachNames$name)))

    coachNames <- data.frame(matrix(ncol = 1, nrow = 0)) # Resets the coaches dataframe
    colnames(coachNames) <- c("name")

    print(plot)
}
```

## Defence Players and Goalkeepers
Let's explore the stats of defence players and goalkeeper overalls in the subset we chose:
```{R}
st(newPlayers[, c("gkOverall", "overall")],
    labels = c("GK Overall", "DF Player Overall"),
    title = "Summary of Defence and Goalkeeper Player Statistics",
    col.align = "right",
    out = "return"
) %>%
    kbl(caption = "Summary of Defence and Goalkeeper Player Statistics") %>%
    kable_styling()
```

We can see some examples of how a goalkeeper's overall changes over time after a defence player joins a team:
```{R, echo = FALSE}

modifiedPlayers["gkName"] <- NA
modifiedPlayers["gkOverall"] <- NA


for (row in list(1, 104)) {
    date <- as.Date(modifiedPlayers[row, "club_joined_date"])
    formattedFifaUpdate <- as.Date(modifiedPlayers$fifa_update_date)
    index <- which.min(abs(formattedFifaUpdate - date))

    filterData <- subset(
        gkPlayers,
        as.Date(fifa_update_date) >= as.Date(modifiedPlayers$fifa_update_date[index]) &
            club_team_id == modifiedPlayers[row, "club_team_id"]
    )

    if (nrow(filterData) != 0) {
        modifiedPlayers[row, ]$gkName <- filterData$short_name[1]
        modifiedPlayers[row, ]$gkOverall <- filterData$overall[1]

        plot <- ggplot(filterData, aes(filterData$fifa_update_date, filterData$overall)) +
            ggtitle("GK Overall as Cause of Defender") +
            xlab("Date") +
            ylab("Overall Rating") +
            geom_point() +
            geom_text_repel(aes(label = paste(filterData$short_name, "/", modifiedPlayers[row, ]$short_name)))

        print(plot)
    }
}
```

# Models
## Coaches and Teams
To explore the impact of predictors on the overall of a team, we will first use a random forest. 
It will run on all predictors, and act as a classifier to see whether the overall of a team is 
higher than the mean of all teams' overalls. The random forest will detail the "importance" of the 
predictors for us, telling us which has the biggest impact on the response.

```{R}
set.seed(1)
split <- 0.7
sample <- sample(nrow(newTeams), split * nrow(newTeams))
trainDataTeams <- newTeams[sample, ]
testDataTeams <- newTeams[-sample, ]
rfTeams <- randomForest(as.factor(overall > mean(overall)) ~ .,
    data = trainDataTeams, ntree = 500, mtry = 7,
    importance = TRUE, na.action = na.exclude)

predictions <- predict(rfTeams, testDataTeams)
cmRF <- confusionMatrix(predictions, as.factor(testDataTeams$overall > mean(testDataTeams$overall)))
print(cmRF)

varImpPlot(rfTeams)

pred2 <- predict(rfTeams, testDataTeams, type = "prob")
pred3 <- prediction(pred2[, 2], as.factor(testDataTeams$overall > mean(testDataTeams$overall)))
auc <- performance(pred3, "auc")@y.values[[1]]
cat("AUC: ", auc)

```

As we can see, coach_id doesn't rank as high as midfield, attack, or defence, meaning coaches might
be less important than we thought. If we explore each predictor on its own in a logistic regression,
we might discover if the random forest was correct.

### Testing midfield strength
First, we run LR on midfield alone as predictor:
```{R}
glmMidfield <- glm(as.factor(overall > mean(overall)) ~ midfield,
    data = trainDataTeams, family = binomial())

midfieldPred <- predict(glmMidfield, testDataTeams)
midfieldPred <- ifelse(midfieldPred >= 0, TRUE, FALSE)

cmMid <- confusionMatrix(as.factor(midfieldPred), as.factor(testDataTeams$overall > mean(testDataTeams$overall)))
print(cmMid)
```
Pretty high accuracy for the predictor alone.

```{R}
pred2 <- predict(glmMidfield, testDataTeams)
pred3 <- prediction(pred2, as.factor(testDataTeams$overall > mean(testDataTeams$overall)))
aucMid <- performance(pred3, "auc")@y.values[[1]]
cat("AUC:", aucMid)

perf <- performance(pred3, "tpr", "fpr")
plot(perf,
    lwd = 2,
    main = "Midfield ROC"
)
abline(a = 0, b = 1)
```
And a good ROC with a high AUC value.


### Testing attack strength
Now we check for attack alone:
```{R}
glmAttack <- glm(as.factor(overall > mean(overall)) ~ attack,
    data = trainDataTeams, family = binomial())

attackPred <- predict(glmAttack, testDataTeams)
attackPred <- ifelse(attackPred >= 0, TRUE, FALSE)

cmAtt <- confusionMatrix(as.factor(attackPred), as.factor(testDataTeams$overall > mean(testDataTeams$overall)))
print(cmAtt)
```
We can see a dip in accuracy here, a 0.88 vs the high 0.92 before.

```{R}
pred2 <- predict(glmAttack, testDataTeams)
pred3 <- prediction(pred2, as.factor(testDataTeams$overall > mean(testDataTeams$overall)))
aucAtt <- performance(pred3, "auc")@y.values[[1]]
cat("AUC:", aucAtt)

perf <- performance(pred3, "tpr", "fpr")
plot(perf,
    lwd = 2,
    main = "Attack ROC"
)
abline(a = 0, b = 1)
```
But the ROC and AUC match midfield's model.

### Testing defence strength
Next we move to defence:

```{R}
glmDefence <- glm(as.factor(overall > mean(overall)) ~ defence,
    data = trainDataTeams, family = binomial()
)

defencePred <- predict(glmDefence, testDataTeams)
defencePred <- ifelse(defencePred >= 0, TRUE, FALSE)

cmDef <- confusionMatrix(as.factor(defencePred), as.factor(testDataTeams$overall > mean(testDataTeams$overall)))
print(cmDef)
```
We see very similar results to midfield's model.


```{R}
pred2 <- predict(glmDefence, testDataTeams)
pred3 <- prediction(pred2, as.factor(testDataTeams$overall > mean(testDataTeams$overall)))
aucDef <- performance(pred3, "auc")@y.values[[1]]
cat("AUC:", aucDef)

perf <- performance(pred3, "tpr", "fpr")
plot(perf,
    lwd = 2,
    main = "Defence ROC"
)
abline(a = 0, b = 1)
```
And a similar ROC and AUC as well.

### Testing coach_id
Because we want to test the coaches' impact over time, we add fifa_update_date, which describes the date
a row was scraped from the games.
```{R}
glmCoach <- glm(as.factor(overall > mean(overall)) ~ coach_id + fifa_update_date,
    data = trainDataTeams, family = binomial()
)

coachPred <- predict(glmCoach, testDataTeams)
coachPred <- ifelse(coachPred >= 0, TRUE, FALSE)

cmCoach <- confusionMatrix(as.factor(coachPred), as.factor(testDataTeams$overall > mean(testDataTeams$overall)))
print(cmCoach)
```
The huge dip in accuracy is apparent here, a 0.62 vs the 0.8's and 0.9's before.

```{R}
pred2 <- predict(glmCoach, testDataTeams)
pred3 <- prediction(pred2, as.factor(testDataTeams$overall > mean(testDataTeams$overall)))
aucCoach <- performance(pred3, "auc")@y.values[[1]]
cat("AUC:", aucCoach)

perf <- performance(pred3, "tpr", "fpr")
plot(perf,
    lwd = 2,
    main = "Coach ROC"
)
abline(a = 0, b = 1)
```
The ROC and AUC don't look good as well. The model seems unable to differentiate between true and false positives/negatives.

## Defence Players and Goalkeepers
To determine the best stats of a defence player related to determining their goalkeeper's overall strength, we will run a 
random forest again with the modified newPlayers dataset and observe the importance of predictors:
```{R}
set.seed(1)
split <- 0.7
sample <- sample(nrow(newPlayers), split * nrow(newPlayers))
trainDataPlayers <- newPlayers[sample, ]
testDataPlayers <- newPlayers[-sample, ]
rfPlayers <- randomForest(as.factor(as.numeric(gkOverall) > mean(as.numeric(gkOverall))) ~ .,
    data = trainDataPlayers, ntree = 500, mtry = 35,
    importance = TRUE
)

predictions <- predict(rfPlayers, testDataPlayers)
confusionMatrix(predictions, as.factor(as.numeric(testDataPlayers$gkOverall)
> mean(as.numeric(testDataPlayers$gkOverall))))

varImpPlot(rfPlayers)

pred2 <- predict(rfPlayers, testDataPlayers, type = "prob")
pred3 <- prediction(pred2[, 2], as.factor(as.numeric(testDataPlayers$gkOverall) > mean(as.numeric(testDataPlayers$gkOverall))))
auc <- performance(pred3, "auc")@y.values[[1]]
cat("AUC: ", auc)
```

```{R}
glmStat <- glm(as.factor(as.numeric(gkOverall) > mean(as.numeric(gkOverall))) ~ as.numeric(rb) + as.numeric(cb) + 
as.numeric(rwb) + as.numeric(lwb) + as.numeric(lb) + as.numeric(lcb) + as.numeric(rcb), data = trainDataPlayers, 
family = binomial())

summary(glmStat)
```
We can see that there is some correlation happening in the model, as evidenced by the "NA" in some of the predictors. Let's
see which predictors they correlate to.

```{R}
cor(sapply(trainDataPlayers[, 96:101], function(x) {
    as.numeric(x)
}))
```
So "lb" and "rb" are correlated, as are "lwb" and "rwb", and "cb" and "lcb" and "rcb". We can reduce the number of predictors
by removing all but one of every correlating group of predictors

```{R}
glmStat <- glm(
    as.factor(as.numeric(gkOverall) > mean(as.numeric(gkOverall))) ~ as.numeric(rb) + as.numeric(cb) +
        as.numeric(rwb),
    data = trainDataPlayers,
    family = binomial()
)

statPred <- predict(glmStat, testDataPlayers)
statPred <- ifelse(statPred >= 0, TRUE, FALSE)
confusionMatrix(as.factor(statPred), as.factor(as.numeric(testDataPlayers$gkOverall) > mean(as.numeric(testDataPlayers$gkOverall)))) # Acc: 0.9243
```

```{R}
pred2 <- predict(glmStat, testDataPlayers)
pred3 <- prediction(pred2, as.factor(as.numeric(testDataPlayers$gkOverall) > mean(as.numeric(testDataPlayers$gkOverall))))
aucDefence1 <- performance(pred3, "auc")@y.values[[1]]
cat("AUC:", aucDefence1)

perf <- performance(pred3, "tpr", "fpr")
plot(perf,
    lwd = 2,
    main = "Defence Player Stats ROC"
)
abline(a = 0, b = 1)
```

# Results and Analysis
## Coaches and Teams
### Summary of Test Data Model Comparision

```{R, echo = FALSE}

teamResults <- data.frame(
   Model = c('Random Forest (All Predictors)', 'LR Midfield', 'LR Attack', 'LR Defence', 'LR Coach (Over Time)'),
   Accuracy = c(cmRF$overall["Accuracy"], cmMid$overall["Accuracy"], cmAtt$overall["Accuracy"], cmDef$overall["Accuracy"], 
   cmCoach$overall['Accuracy']),
   AUC = c(auc, aucMid, aucAtt, aucDef, aucCoach),
   PValue = c(
       cmRF$overall["McnemarPValue"], cmMid$overall["McnemarPValue"], cmAtt$overall["McnemarPValue"], cmDef$overall["McnemarPValue"],
       cmCoach$overall["McnemarPValue"]
   )
)

teamResults %>%
kbl() %>%
kable_styling()
```

As we can see, the accuracies for midfield, attack, and defence classifiers were much higher than the coach over-time
classifier, in the range of 80-90%. The result may be due to the distribution of quantitative values in the former three as 
opposed to the two categorical predictors coach_id and fifa_update_date.

But it brings to mind a question: as a coach, they strategize plans and formations for the team. Wouldn't the coach have
a hand in a team's defence, midfield, and attack strength? It's hard to answer without more data to follow through with. With
the data we have on hand, we can conclde temporarily for our first two questions: **Defence, Midfield, and Attack can help predict
a team's overall performance better than who the coach is. The three stats can be realted to how the coach delegates his team.**

## Defence Players and Goalkeepers





# Impact
The data could be used in several ways:

1. Choosing coaches based on their improvement of previous teams.
This model will not change how coaches are determined in football, but compliment it. A coach's job is to improve 
a team's performance during their tenure. A manager might use this model to fasten their decision in continuing 
with a certain coach or choosing which coach to bring next based on past achievements.

2. Choosing defence players based on past synergy with goalkeepers.
A team manager would probably buy and sell their players based on whether they work well with the goalkeepers or
not. They could compare the stats of a goalkeeper when a defender joined, and their stats when the two players 
separate and determine a defence player's worth based on that.

3. Betting on matches and players.
The model's could help predict slightly the performance of a goalkeeper or a team with a certain coach. We say
slightly because the classifiers check if the overall is higher than the mean overall. That might not help betters
a lot, but it still could be considered unethical to publish without at least a disclaimer or use privately with
football managers.
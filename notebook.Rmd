
```{R, include=FALSE}
library(knitr)
knitr::opts_chunk$set(warning = FALSE)
```
# Authors
* Aziz Alabduljalil
* Abduarraheem Elfandi

## Disclaimer
Each section is divided between two subsections: Coaches and Teams, and Defence Players and Goalkeepers. The first subsection will explore the first and second questions, while the second subsection explores the third and fourth questions.

## Install libraries
```{R, eval = FALSE}
install.packages("caret")
install.packages("glmnet")
install.packages("tidyverse")
install.packages("ggrepel")
install.packages("randomForest")
install.packages("ROCR")
install.packages("kableExtra")
install.packages("vtable")
install.packages("corrplot")
```

## Load libraries
```{R, message = FALSE}
library("caret")
library("glmnet")
library("tidyverse")
library("dplyr")
library("ggplot2")
library("ggrepel")
library("randomForest")
library("ROCR")
library("kableExtra")
library("vtable")
library("corrplot")
require(data.table)
```

# Data
Our data consists of three datasets: "male_teams.csv", "male_players (legacy).csv", and "male_coaches.csv". It contains data on national and club teams from 2015-2023. The data is scraped from the FIFA games produced annually. While our research concerns real-life players and teams, the stats in the games require teams of observers watching real matches for months to decide. Thus, we will treat the data as if it applies to the real players and teams.

The dataset can be found at https://www.kaggle.com/datasets/stefanoleone992/fifa-23-complete-player-dataset.

## Load datasets
```{R, eval = TRUE}
teams <- read.csv("./male_teams.csv")
players <- read.csv("./male_players (legacy).csv")
coaches <- read.csv("./male_coaches.csv")
```

## Processing
### Teams
For the teams dataset, we wanted to focus on teams who had more than one coach to gauge the impact each coach left on the teams overall stats.

Before using any of the data, any columns with NA values in them were disregarded to not impede the models later on. The Coach_ID column from teams dataset, for example, which is prevelant to answer our questions, instead had its NA values replaced with the median of the column.

As such, the teams dataset was reduced from 54 columns to the 27 listed below:

```{R}
newTeams <- data.frame(matrix(ncol = 54, nrow = 0))
colnames(newTeams) <- names(teams)
for (team in unique(teams$team_id)) {
    team_df <- subset(teams, team_id == team)

    if (length(unique(team_df$coach_id)) > 1) {
        newTeams <- rbind(newTeams, team_df)
    }
}

# Replaces NA values in coach_id column with a random coach_id
newTeams$coach_id[is.na(newTeams$coach_id)] <- sample.int(37666625, 1, replace = TRUE)

newTeams <- newTeams[, colSums(is.na(newTeams)) == 0] # Remove all columns with any NA values in them
```

``` {R, echo = FALSE}
knitr::kable(names(newTeams), format = "markdown")
```

### Players
The same was done with the players dataset. To prevent using a large dataset, players were reduced to the first 3000 unique defence players. With each player having an entry for each FIFA update, the dataset grows large. each row then had that defence player's goalkeeper and goalkeeper overall for that FIFA update to help our models later answer our thrid and fourth questions.

The same treatment of omitting NA columns was done to the players dataset, leaving 105 columns as shown below:
```{R}
modifiedPlayers <- subset(players, club_position %in%
    c("CB", "LB", "RB", "RWB", "LWB"))

modifiedPlayers <- subset(modifiedPlayers, short_name %in%
    unique(modifiedPlayers$short_name)[1:3000])

gkPlayers <- subset(players, club_position == "GK")

modifiedPlayers["gkName"] <- NA
modifiedPlayers["gkOverall"] <- NA


for (row in seq_len(nrow(modifiedPlayers))) {
    date <- as.Date(modifiedPlayers[row, "club_joined_date"])
    formattedFifaUpdate <- as.Date(modifiedPlayers$fifa_update_date)
    index <- which.min(abs(formattedFifaUpdate - date))

    filterData <- subset(
        gkPlayers,
        as.Date(fifa_update_date) >= as.Date(modifiedPlayers$fifa_update_date[index]) &
            club_team_id == modifiedPlayers[row, "club_team_id"]
    )

    if (nrow(filterData) != 0) {
        modifiedPlayers[row, ]$gkName <- filterData$short_name[1]
        modifiedPlayers[row, ]$gkOverall <- filterData$overall[1]
    }
}

# Replaces NA values in gkOverall column with median of gkOverall
val <- median(na.omit(modifiedPlayers$gkOverall))
modifiedPlayers$gkOverall[is.na(modifiedPlayers$gkOverall)] <- val

modifiedPlayers$gkName[is.na(modifiedPlayers$gkName)] <- "None"

newPlayers <- modifiedPlayers[, colSums(is.na(modifiedPlayers)) == 0] # Remove all columns with any NA values in them

# Remove arithmetic signs from numeric columns
newPlayers[] <- lapply(newPlayers, function(x) {
    gsub("[-|\\+].*", "", x)
})
```

``` {R, echo = FALSE}
knitr::kable(names(newPlayers), format = "markdown")
```
For more detail on what each column represents, please follow the dataset link.

# Questions
1. Do coaches have a big impact on the overall performance of the team they coach?
2. If not, what predictors do, and are they related to coaches at all?
3. Does a defence player's performance have a large impact on their goalkeeper's performance?
4. If not, which predictors do?

# Visualizations
## Coaches and Teams
Let's first explore the current overall of the teams in the dataset, where overall combines their attack, midfield, and defence stats into one.

```{R}
st(teams[, c("overall", "attack", "midfield", "defence")],
    labels = c("Overall", "Attack", "Midfield", "Defence"),
    title = "Summary of Team Statistics",
    col.align = "right",
    out = "return"
) %>%
    kbl(caption = "Summary of Team Statistics") %>%
    kable_styling()
```

To see how the teams' overall changes over the tenure of their different coaches, we plot the teams' overalls throughout. Here we see an example of Villarreal and Aston Villa teams:

```{R, echo = FALSE}
coachNames <- data.frame(matrix(ncol = 1, nrow = 0))
colnames(coachNames) <- c("name")

for (team in list("Villarreal", "Aston Villa")) {
    team_df <- subset(teams, team_name == team)

    if (length(unique(team_df$coach_id)) <= 1) {
        next
    }

    finalTeamDF <- data.frame(matrix(ncol = 54, nrow = 0))
    colnames(finalTeamDF) <- names(team_df)

    for (i in seq_len(nrow(team_df))) { # Now makes a list of all names first, then uses it in plot
        sname <- c(coaches[which(team_df[i, ]$coach_id == coaches$coach_id), ]$short_name)

        if (identical(sname, character(0))) {
            next
        } else if (nrow(coachNames) == 0) {
            finalTeamDF <- rbind(finalTeamDF, team_df[i, ])

            coachNames[nrow(coachNames) + 1, ] <-
                c(coaches[which(team_df[i, ]$coach_id == coaches$coach_id), ]$short_name)
        } else if (i == nrow(team_df)) {
            finalTeamDF <- rbind(finalTeamDF, team_df[i, ])

            coachNames[nrow(coachNames) + 1, ] <-
                c(coaches[which(team_df[i, ]$coach_id == coaches$coach_id), ]$short_name)
        } else if (tail(coachNames, 1)[1, 1] == sname & nrow(coachNames) > 0) {
            next
        } else {
            finalTeamDF <- rbind(finalTeamDF, team_df[i - 1, ])
            coachNames[nrow(coachNames) + 1, ] <-
                c(coaches[which(team_df[i - 1, ]$coach_id == coaches$coach_id), ]$short_name)

            finalTeamDF <- rbind(finalTeamDF, team_df[i, ])
            coachNames[nrow(coachNames) + 1, ] <-
                c(coaches[which(team_df[i, ]$coach_id == coaches$coach_id), ]$short_name)
        }
    }

    plot <- ggplot(finalTeamDF, aes(finalTeamDF$fifa_update_date, finalTeamDF$overall)) +
        ggtitle("Coach and Team Performance Overall") +
        xlab("Date") +
        ylab("Overall Rating of Team") +
        geom_point() +
        geom_text_repel(aes(label = paste(finalTeamDF$team_name, "/", coachNames$name)))

    coachNames <- data.frame(matrix(ncol = 1, nrow = 0)) # Resets the coaches dataframe
    colnames(coachNames) <- c("name")

    print(plot)
}
```
Here the x-axis shows the date the data was collected, and the y-axis shows the overall rating of the team. The format of labeling is (Team/Coach), and shows the points where a coach joined the team and when they left to compare ratings at both time periods.

As we can see, in the first plot, Villarreal ended 2014 with an overall rating of 76. That's when coach U. Emery joined the team. When he left the team in July 2022, the team's rating rose to 80 increasing 4 points under his tenure. His successor E. Seiten Solar, who only coached the team a few months so far, maintained the rating.

In the second plot, Aston Villa ended 2014 with a rating of 74 when S. Gerrard joined as coach. When he left in October 2022, the team's rating rested at 79, increasing 5 points under his tenure. U. Emery then joined recently, so has not changed the team's rating at all.

## Defence Players and Goalkeepers
Let's explore the stats of defence players and goalkeeper overalls in the subset we chose:
```{R}
newPlayers$gkOverall <- as.numeric(newPlayers$gkOverall)
newPlayers$overall <- as.numeric(newPlayers$overall)

st(newPlayers[, c("gkOverall", "overall")],
    labels = c("GK Overall", "DF Player Overall"),
    title = "Summary of Defence and Goalkeeper Player Statistics",
    col.align = "right",
    out = "return"
) %>%
    kbl(caption = "Summary of Defence and Goalkeeper Player Statistics") %>%
    kable_styling()
```

We can see some examples of how a goalkeeper's overall changes over time after a defence player joins a team:
```{R, echo = FALSE}

modifiedPlayers["gkName"] <- NA
modifiedPlayers["gkOverall"] <- NA


for (row in list(1, 104)) {
    date <- as.Date(modifiedPlayers[row, "club_joined_date"])
    formattedFifaUpdate <- as.Date(modifiedPlayers$fifa_update_date)
    index <- which.min(abs(formattedFifaUpdate - date))

    filterData <- subset(
        gkPlayers,
        as.Date(fifa_update_date) >= as.Date(modifiedPlayers$fifa_update_date[index]) &
            club_team_id == modifiedPlayers[row, "club_team_id"]
    )

    if (nrow(filterData) != 0) {
        modifiedPlayers[row, ]$gkName <- filterData$short_name[1]
        modifiedPlayers[row, ]$gkOverall <- filterData$overall[1]

        plot <- ggplot(filterData, aes(filterData$fifa_update_date, filterData$overall)) +
            ggtitle("GK Overall as Cause of Defender") +
            xlab("Date") +
            ylab("Overall Rating of Goalkeeper]") +
            geom_point() +
            geom_text_repel(aes(label = paste(filterData$short_name, "/", modifiedPlayers[row, ]$short_name)))

        print(plot)
    }
}
```
Like in the previous section, the x-axis shows the date the data was collected, and the y-axis shows the overall rating of the goalkeeper. The format for the point labels is (Goalkeeper Name/Defender Name), and shows the points where a goalkeeper joins the defender's team and when they leave to compare the overall rating at both time periods.

In the first plot, the goalkeeper M. Neuer joins the defender J. Boateng's team in 2014 with an overall rating of 90. In September 2019, Neuer's rating increases to 92 for two years before crashing down to 88 in September 2019. He then raises his rating back to his original 90 in September 2021 where it remains till today. Whether J. Boateng's performance as defender influenced this change remains to be seen.

In the second plot, the goalkeeper K. Trapp leaves the defender C. Djakpa's team and is replaced with the goalkeeper L. Hradecky, who joins with an overall rating of 70. When he leaves in September 2017, L. Hradecky's rating rises to 81. F. Ronnow doesn't stay in the team long enough for data to be collected before K. Trapp returns in September 2019 with a rating of 83. He stays with the team till the current day, where his rating increased to 86. C. Djakpa's influence as a defender on his goalkeepers' ratings will be analyzed later.

# Models
## Coaches and Teams
To explore the impact of predictors on the overall of a team, we will first use a random forest. It will run on all predictors, and act as a classifier to see whether the overall of a team is higher than the mean of all teams' overalls. The random forest will detail the "importance" of the predictors for us, telling us which has the biggest impact on the response.

```{R}
set.seed(1)
split <- 0.7
sample <- sample(nrow(newTeams), split * nrow(newTeams))
trainDataTeams <- newTeams[sample, ]
testDataTeams <- newTeams[-sample, ]
rfTeams <- randomForest(as.factor(overall > mean(overall)) ~ .,
    data = trainDataTeams, ntree = 500, mtry = 7,
    importance = TRUE, na.action = na.exclude)

predictions <- predict(rfTeams, testDataTeams)
cmRF <- confusionMatrix(predictions, as.factor(testDataTeams$overall > mean(testDataTeams$overall)))
print(cmRF)

varImpPlot(rfTeams)

pred2 <- predict(rfTeams, testDataTeams, type = "prob")
pred3 <- prediction(pred2[, 2], as.factor(testDataTeams$overall > mean(testDataTeams$overall)))
auc <- performance(pred3, "auc")@y.values[[1]]
cat("AUC: ", auc)

```

As we can see, coach_id doesn't rank as high as midfield, attack, or defence, meaning coaches might be less important than we thought. If we explore each predictor on its own in a logistic regression, we might discover if the random forest was correct.

For logistic regression, we will try to classify whether a team's overall rating is higher than the mean team overall rating or not. The following models all use the same response, but use different predictors as explained in their titles.

### Logistic Regression: Testing midfield strength
First, we run LR on midfield alone as predictor:

```{R}
glmMidfield <- glm(as.factor(overall > mean(overall)) ~ midfield,
    data = trainDataTeams, family = binomial())

midfieldPred <- predict(glmMidfield, testDataTeams)
midfieldPred <- ifelse(midfieldPred >= 0, TRUE, FALSE)

cmMid <- confusionMatrix(as.factor(midfieldPred), as.factor(testDataTeams$overall > mean(testDataTeams$overall)))
print(cmMid)
```
Pretty high accuracy for the predictor alone, a 92%.

```{R}
pred2 <- predict(glmMidfield, testDataTeams)
pred3 <- prediction(pred2, as.factor(testDataTeams$overall > mean(testDataTeams$overall)))
aucMid <- performance(pred3, "auc")@y.values[[1]]
cat("AUC:", aucMid)

perf <- performance(pred3, "tpr", "fpr")
plot(perf,
    lwd = 2,
    main = "Midfield ROC"
)
abline(a = 0, b = 1)
```
And a good ROC with a high AUC value.


### Logistic Regression: Testing attack strength
Now we check for attack alone:
```{R}
glmAttack <- glm(as.factor(overall > mean(overall)) ~ attack,
    data = trainDataTeams, family = binomial())

attackPred <- predict(glmAttack, testDataTeams)
attackPred <- ifelse(attackPred >= 0, TRUE, FALSE)

cmAtt <- confusionMatrix(as.factor(attackPred), as.factor(testDataTeams$overall > mean(testDataTeams$overall)))
print(cmAtt)
```
We can see a dip in accuracy here, a 88% vs the high 92% before.

```{R}
pred2 <- predict(glmAttack, testDataTeams)
pred3 <- prediction(pred2, as.factor(testDataTeams$overall > mean(testDataTeams$overall)))
aucAtt <- performance(pred3, "auc")@y.values[[1]]
cat("AUC:", aucAtt)

perf <- performance(pred3, "tpr", "fpr")
plot(perf,
    lwd = 2,
    main = "Attack ROC"
)
abline(a = 0, b = 1)
```
But the ROC and AUC match midfield's model.

### Logistic Regression: Testing defence strength
Next we move to defence:

```{R}
glmDefence <- glm(as.factor(overall > mean(overall)) ~ defence,
    data = trainDataTeams, family = binomial()
)

defencePred <- predict(glmDefence, testDataTeams)
defencePred <- ifelse(defencePred >= 0, TRUE, FALSE)

cmDef <- confusionMatrix(as.factor(defencePred), as.factor(testDataTeams$overall > mean(testDataTeams$overall)))
print(cmDef)
```
We see very similar results to midfield's model.


```{R}
pred2 <- predict(glmDefence, testDataTeams)
pred3 <- prediction(pred2, as.factor(testDataTeams$overall > mean(testDataTeams$overall)))
aucDef <- performance(pred3, "auc")@y.values[[1]]
cat("AUC:", aucDef)

perf <- performance(pred3, "tpr", "fpr")
plot(perf,
    lwd = 2,
    main = "Defence ROC"
)
abline(a = 0, b = 1)
```
And a similar ROC and AUC as well.

### Logistic Regression: Testing coach_id
Because we want to test the coaches' impact over time, we add fifa_update_date, which describes the date a row was scraped from the games.

```{R}
glmCoach <- glm(as.factor(overall > mean(overall)) ~ coach_id + fifa_update_date,
    data = trainDataTeams, family = binomial()
)

coachPred <- predict(glmCoach, testDataTeams)
coachPred <- ifelse(coachPred >= 0, TRUE, FALSE)

cmCoach <- confusionMatrix(as.factor(coachPred), as.factor(testDataTeams$overall > mean(testDataTeams$overall)))
print(cmCoach)
```
The huge dip in accuracy is apparent here, a 62% vs the 80's and 90's before.

```{R}
pred2 <- predict(glmCoach, testDataTeams)
pred3 <- prediction(pred2, as.factor(testDataTeams$overall > mean(testDataTeams$overall)))
aucCoach <- performance(pred3, "auc")@y.values[[1]]
cat("AUC:", aucCoach)

perf <- performance(pred3, "tpr", "fpr")
plot(perf,
    lwd = 2,
    main = "Coach ROC"
)
abline(a = 0, b = 1)
```
The ROC and AUC don't look good as well. The model seems unable to differentiate between true and false positives/negatives.

## Defence Players and Goalkeepers
To determine the best stats of a defence player related to determining their goalkeeper's overall strength, we will run a random forest again with the modified newPlayers dataset and observe the importance of predictors:

### Random Forest
```{R}
set.seed(1)
split <- 0.7
sample <- sample(nrow(newPlayers), split * nrow(newPlayers))
trainDataPlayers <- newPlayers[sample, ]
testDataPlayers <- newPlayers[-sample, ]
rfPlayers <- randomForest(as.factor(as.numeric(gkOverall) > mean(as.numeric(gkOverall))) ~ .,
    data = trainDataPlayers, ntree = 500, mtry = 35,
    importance = TRUE
)

predictions <- predict(rfPlayers, testDataPlayers)
cmRFPlayers <- confusionMatrix(predictions, as.factor(as.numeric(testDataPlayers$gkOverall)
> mean(as.numeric(testDataPlayers$gkOverall))))
print(cmRFPlayers)

varImpPlot(rfPlayers)

pred2 <- predict(rfPlayers, testDataPlayers, type = "prob")
pred3 <- prediction(pred2[, 2], as.factor(as.numeric(testDataPlayers$gkOverall) > mean(as.numeric(testDataPlayers$gkOverall))))
aucRFPlayers <- performance(pred3, "auc")@y.values[[1]]
cat("AUC: ", aucRFPlayers)
```
A lot of the important predictors are categorical, which makes logistic regression harder with them as predictors. Instead, we will use the highest quantitative variables: "overall", overall rating of the defence player; "rb, cb, lb, lwb, rwb, lcb, rcb", the individual stats of the defence player if they played in those positions to predict the goalkeeper's overall rating.

For logistic regression, we will try to classify whether a goalkeeper's overall rating is higher than the mean goalkeeper overall rating or not using defender stats as predictors. The following models all use the same response, but use different predictors as explained in their titles.

### Logistic Regression: Testing Individual Defence Ratings
```{R}
glmStat <- glm(as.factor(as.numeric(gkOverall) > mean(as.numeric(gkOverall))) ~ as.numeric(rb) + as.numeric(cb) + 
as.numeric(rwb) + as.numeric(lwb) + as.numeric(lb) + as.numeric(lcb) + as.numeric(rcb), data = trainDataPlayers, 
family = binomial())

summary(glmStat)
```
First we use a defender's individual stats if they played in those positions to classify the goalkeeper's overall rating. We can see that there is some correlation happening in the model, as evidenced by the "NA" in some of the predictors. Let's see which predictors they correlate to.

```{R}
corGraph <- cor(sapply(trainDataPlayers[, c(92, 96:101)], function(x) {
    as.numeric(x)
}))

colnames(corGraph) <- names(trainDataPlayers[, c(92, 96:101)])
rownames(corGraph) <- names(trainDataPlayers[, c(92, 96:101)])
corrplot(corGraph)

```
So "lb" and "rb" are linearly correlated, as are "lwb" and "rwb", and "cb" and "lcb" and "rcb". We can reduce the number of predictors by removing all but one of every correlating group of predictors. Once we train again, it will be without any redundancies.

```{R}
glmStat <- glm(
    as.factor(as.numeric(gkOverall) > mean(as.numeric(gkOverall))) ~ as.numeric(rb) + as.numeric(cb) +
        as.numeric(rwb),
    data = trainDataPlayers,
    family = binomial()
)

statPred <- predict(glmStat, testDataPlayers)
statPred <- ifelse(statPred >= 0, TRUE, FALSE)
cmPosition <- confusionMatrix(as.factor(statPred), as.factor(as.numeric(testDataPlayers$gkOverall) > mean(as.numeric(testDataPlayers$gkOverall))))
print(cmPosition)
```
The accuracy is lower than we imagined, with 76% accuracy.

```{R}
pred2 <- predict(glmStat, testDataPlayers)
pred3 <- prediction(pred2, as.factor(as.numeric(testDataPlayers$gkOverall) > mean(as.numeric(testDataPlayers$gkOverall))))
aucDefence1 <- performance(pred3, "auc")@y.values[[1]]
cat("AUC:", aucDefence1)

perf <- performance(pred3, "tpr", "fpr")
plot(perf,
    lwd = 2,
    main = "Defence Player Stats ROC"
)
abline(a = 0, b = 1)
```
The AUC and ROC are not terrible, but maybe including a different stat will result in a better model. Using the overall of the defence player instead of their individual stats in particular positions would make a good second model.

### Logistic Regression: Testing Defence Player Overall
```{R}
glmOverall <- glm(as.factor(as.numeric(gkOverall) > mean(as.numeric(gkOverall))) ~ as.numeric(overall), data = trainDataPlayers, family = binomial())

overallPred <- predict(glmOverall, testDataPlayers)
overallPred <- ifelse(overallPred >= 0, TRUE, FALSE)
cmPlayerOverall <- confusionMatrix(as.factor(overallPred), as.factor(as.numeric(testDataPlayers$gkOverall) > mean(as.numeric(testDataPlayers$gkOverall))))
print(cmPlayerOverall)
```
Here, **"gkOverall" is the overall rating of the goalkeeper, while "overall" is the overall rating of the defender.** We got a slightly better accuracy with an increase of 1%.

```{R}
pred2 <- predict(glmOverall, testDataPlayers)
pred3 <- prediction(pred2, as.factor(as.numeric(testDataPlayers$gkOverall) > mean(as.numeric(testDataPlayers$gkOverall))))
aucDefence2 <- performance(pred3, "auc")@y.values[[1]]
cat("AUC:", aucDefence2)

perf <- performance(pred3, "tpr", "fpr")
plot(perf,
    lwd = 2,
    main = "Defence Player Overall ROC"
)
abline(a = 0, b = 1)
```
And also a very small increase in AUC. Using overall better encapsulates the abilties of a player than individual positions, which may have lead to this increase. Lastly, adding the date a player joins the team might affect the accuracy.

### Logistic Regression: Testing Defence Player Overall and Date Joined Club
```{R}
glmOverallDate <- glm(as.factor(as.numeric(gkOverall) > mean(as.numeric(gkOverall))) ~ as.numeric(overall) + club_joined_date, data = trainDataPlayers, family = binomial())
summary(glmOverallDate)

overallPredDate <- predict(glmOverallDate, testDataPlayers)
overallPredDate <- ifelse(overallPredDate >= 0, TRUE, FALSE)
cmOverallDate <- confusionMatrix(as.factor(overallPredDate), as.factor(as.numeric(testDataPlayers$gkOverall) > mean(as.numeric(testDataPlayers$gkOverall))))
print(cmOverallDate)
```
Now we add as predictor the date the defender joins the goalkeeper's team in addition to the defender's overall rating. The accuracy improves a bit with the new predictor.

```{R}
pred2 <- predict(glmOverallDate, testDataPlayers)
pred3 <- prediction(pred2, as.factor(as.numeric(testDataPlayers$gkOverall) > mean(as.numeric(testDataPlayers$gkOverall))))
aucDefence3 <- performance(pred3, "auc")@y.values[[1]]
cat("AUC:", aucDefence3)

perf <- performance(pred3, "tpr", "fpr")
plot(perf,
    lwd = 2,
    main = "Defence Player Overall + Club Joined Date ROC"
)
abline(a = 0, b = 1)
```
But reports similar AUC and ROC.

# Results and Analysis
## Coaches and Teams
### Summary of Test Data Model Comparision

```{R, echo = FALSE}

teamResults <- data.frame(
    Model = c('Random Forest (All Predictors)', 'LR Midfield', 'LR Attack', 'LR Defence', 'LR Coach (Over Time)'),
    Accuracy = c(cmRF$overall["Accuracy"], cmMid$overall["Accuracy"], cmAtt$overall["Accuracy"], cmDef$overall["Accuracy"], 
    cmCoach$overall['Accuracy']),
    AUC = c(auc, aucMid, aucAtt, aucDef, aucCoach),
    Precision = c(cmRF$byClass["Precision"], cmMid$byClass["Precision"], cmAtt$byClass["Precision"], cmDef$byClass["Precision"],
        cmCoach$byClass["Precision"]),
    Recall = c(cmRF$byClass["Recall"], cmMid$byClass["Recall"], cmAtt$byClass["Recall"], cmDef$byClass["Recall"],
        cmCoach$byClass["Recall"]),
    F-1 Score = c(cmRF$byClass["F1"], cmMid$byClass["F1"], cmAtt$byClass["F1"], cmDef$byClass["F1"],
        cmCoach$byClass["F1"])
)

teamResults %>%
kbl() %>%
kable_styling()
```

As we can see, the accuracies for midfield, attack, and defence classifiers were much higher than the coach over-time classifier, in the range of 80-90%. The result may be due to the distribution of quantitative values in the former three as opposed to the two categorical predictors coach_id and fifa_update_date.

But it brings to mind a question: as a coach, they strategize plans and formations for the team. Wouldn't the coach have a hand in a team's defence, midfield, and attack strength? It's hard to answer without more data to follow through with. With the data we have on hand, we can conclde temporarily for our first two questions: **Defence, Midfield, and Attack can help predict a team's overall performance better than who the coach is. The three stats can be related to how the coach delegates his team, and would need more in-depth data to study.**

## Defence Players and Goalkeepers
### Summary of Test Data Model Comparision
```{R, echo = FALSE}

playerResults <- data.frame(
   Model = c('Random Forest (All Predictors)', 'LR Defender Position Stats', 'LR Defender Overall', 'LR Defender Overall + Joined Date'),
   Accuracy = c(cmRFPlayers$overall["Accuracy"], cmPosition$overall["Accuracy"], cmPlayerOverall$overall["Accuracy"], cmOverallDate$overall["Accuracy"]),
   AUC = c(aucRFPlayers, aucDefence1, aucDefence2, aucDefence3),
   Precision = c(cmRFPlayers$byClass["Precision"], cmPosition$byClass["Precision"], cmPlayerOverall$byClass["Precision"], cmOverallDate$byClass["Precision"]),
   Recall = c(cmRFPlayers$byClass["Recall"], cmPosition$byClass["Recall"], cmPlayerOverall$byClass["Recall"], cmOverallDate$byClass["Recall"]),
   F-1 Score = c(cmRFPlayers$byClass["F1"], cmPosition$byClass["F1"], cmPlayerOverall$byClass["F1"], cmOverallDate$byClass["F1"])
)

playerResults %>%
kbl() %>%
kable_styling()
```

As we can see, using the "overall" stat was better than using a player's stats when in specific defending positions. The accuracy doesn't help a lot in answering our question. A defender can classify a goalkeeper's performance 77% of the time, which is not high enough to conclusively make our hypothesis true. Adding in when the defender joined the team increased the accuracy a little, but still not enough for our question.

The random forest showed a lot more important variables before a defender's stats, but they were all categorical. This makes me believe which team the goalkeeper joined had a bigger impact on their overall than a defender's performance ("club_team_id", "club_joined_date", "club_name"...). 
 
It leads us to conclude that for our third and fourth questions: **While a defender's stats and time they joined the club can impact the goalkeeper's performance sufficiently, the team the goalkeeper ends up joining has a bigger impact.**


# Impact
The data could be used in several ways:

1. Choosing coaches based on their improvement of previous teams.
This model will not change how coaches are determined in football, but compliment it. A coach's job is to improve a team's performance during their tenure. A manager might use this model to fasten their decision in continuing with a certain coach or choosing which coach to bring next based on past achievements.

2. Choosing defence players based on past synergy with goalkeepers.
A team manager would probably buy and sell their players based on whether they work well with the goalkeepers or not. They could compare the stats of a goalkeeper when a defender joined, and their stats when the two players separate and determine a defence player's worth based on that.

3. Betting on matches and players.
The model's could help predict slightly the performance of a goalkeeper or a team with a certain coach. We say slightly because the classifiers check if the overall is higher than the mean overall. That might not help betters a lot, but it still could be considered unethical to publish without at least a disclaimer or use privately with football managers.